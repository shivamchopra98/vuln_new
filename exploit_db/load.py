# load.py
import os
import math
import json
import time
import pandas as pd
import boto3
from decimal import Decimal
from botocore.exceptions import ClientError

# Configuration (leave as-is or pass config from exploit_main later)
TABLE_NAME = "exploit_data"
DDB_ENDPOINT = "http://localhost:8000"
AWS_REGION = "us-east-1"
PROJECT_ROOT = r"C:\Users\ShivamChopra\Projects\vuln\exploit_db"
DAILY_DIR = os.path.join(PROJECT_ROOT, "daily_extract")
BASELINE_FILE = os.path.join(DAILY_DIR, "exploit_extract.csv")
BATCH_PROGRESS_INTERVAL = 100

# ---------- Helpers ----------
def ensure_daily_dir():
    os.makedirs(DAILY_DIR, exist_ok=True)

def get_table():
    dynamodb = boto3.resource(
        "dynamodb",
        region_name=AWS_REGION,
        aws_access_key_id="dummy",
        aws_secret_access_key="dummy",
        endpoint_url=DDB_ENDPOINT,
    )
    existing = dynamodb.meta.client.list_tables().get("TableNames", [])
    if TABLE_NAME not in existing:
        print(f"‚ö° Creating DynamoDB table '{TABLE_NAME}' locally...")
        table = dynamodb.create_table(
            TableName=TABLE_NAME,
            KeySchema=[{"AttributeName": "id", "KeyType": "HASH"}],
            AttributeDefinitions=[{"AttributeName": "id", "AttributeType": "S"}],
            ProvisionedThroughput={"ReadCapacityUnits": 5, "WriteCapacityUnits": 5},
        )
        table.meta.client.get_waiter("table_exists").wait(TableName=TABLE_NAME)
        print("‚úÖ Table created.")
    else:
        table = dynamodb.Table(TABLE_NAME)
    return table

def normalize_value(v):
    """Normalize value for robust comparison"""
    if v is None:
        return None
    # pandas NaN handling
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    # Decimal
    if isinstance(v, Decimal):
        return str(v)
    # float
    if isinstance(v, float):
        if math.isnan(v) or math.isinf(v):
            return None
        s = str(v)
        if s.endswith(".0"):
            return s[:-2]
        return s
    s = str(v).strip()
    if s == "" or s.lower() in {"nan", "none"}:
        return None
    return s

def normalize_row(row_dict):
    """Return normalized dict excluding uploaded_date (ignored for comparison)"""
    return {k: normalize_value(v) for k, v in row_dict.items() if k != "uploaded_date"}

def rows_differ(csv_row, ddb_item):
    """Compare normalized versions (ignore uploaded_date)."""
    return normalize_row(csv_row) != normalize_row(ddb_item)

def write_uploaded_ids_file(ids, tag):
    os.makedirs(DAILY_DIR, exist_ok=True)
    path = os.path.join(DAILY_DIR, f"uploaded_ids_{tag}.txt")
    with open(path, "w", encoding="utf-8") as f:
        for _id in ids:
            f.write(f"{_id}\n")
    print(f"‚ÑπÔ∏è Wrote uploaded ids: {path}")

# ---------- Main exported function ----------
def sync_today_with_dynamodb(current_csv_path: str):
    """
    Sync behaviour:
      - Compare incoming transformed CSV with existing baseline (if present) to compute changed_ids.
      - Ensure baseline rows missing in DynamoDB are re-added.
      - Upload only true missing/changed rows to DynamoDB.
      - Overwrite baseline file so only exploit_extract.csv exists in daily_extract/.
      - Return a summary dict and write a sync_log JSON and uploaded_ids TXT.
    """
    ensure_daily_dir()
    table = get_table()

    # --- Load incoming CSV (transformed)
    df_new = pd.read_csv(current_csv_path, dtype=str)
    new_count = len(df_new)
    print(f"‚ÑπÔ∏è Incoming transformed rows: {new_count}")

    # Build id -> row map from new file
    new_map = {}
    for _, r in df_new.iterrows():
        rid = r.get("id")
        if pd.isna(rid) or str(rid).strip() == "":
            continue
        new_map[str(rid).strip()] = r.to_dict()

    # --- Load baseline if exists
    baseline_exists = os.path.exists(BASELINE_FILE)
    base_map = {}
    if baseline_exists:
        df_base = pd.read_csv(BASELINE_FILE, dtype=str)
        for _, r in df_base.iterrows():
            rid = r.get("id")
            if pd.isna(rid) or str(rid).strip() == "":
                continue
            base_map[str(rid).strip()] = r.to_dict()
        print(f"‚ÑπÔ∏è Baseline found with {len(base_map)} rows")
    else:
        print("‚ÑπÔ∏è No baseline found (first run)")

    # --- Compute changed_ids by comparing new_map vs base_map (ignore uploaded_date)
    changed_ids = []
    for rid, new_row in new_map.items():
        base_row = base_map.get(rid)
        if base_row is None:
            changed_ids.append(rid)
        else:
            if rows_differ(new_row, base_row):
                changed_ids.append(rid)

    # --- Additionally check baseline rows missing from DynamoDB (re-add deleted rows)
    # We'll fetch from DynamoDB only for baseline ids (to detect deletions).
    missing_from_ddb_ids = []
    if baseline_exists:
        # For efficiency, fetch per-id for baseline (could be optimized in batches)
        for rid in base_map.keys():
            try:
                resp = table.get_item(Key={"id": rid})
                if "Item" not in resp:
                    missing_from_ddb_ids.append(rid)
            except ClientError as e:
                print(f"‚ö†Ô∏è Warning fetching id={rid} from DDB: {e}")
    # merge missing ids so they will be written
    for mid in missing_from_ddb_ids:
        if mid not in changed_ids:
            changed_ids.append(mid)

    # If no changes at all, we still overwrite baseline with incoming file (per requirement)
    if not changed_ids:
        print("‚úÖ No changes detected vs baseline and no missing rows in DDB.")
    else:
        print(f"‚ÑπÔ∏è Total changed/missing ids to consider: {len(changed_ids)}")

    # --- Prepare items to write by checking each changed_id against DynamoDB
    to_write = []
    for rid in changed_ids:
        csv_row = new_map.get(rid) or base_map.get(rid)
        if csv_row is None:
            continue
        # normalize for id and clean
        csv_row_prepared = {}
        for k, v in csv_row.items():
            if pd.isna(v) or (isinstance(v, str) and v.strip() == ""):
                csv_row_prepared[k] = None
            else:
                csv_row_prepared[k] = v
        csv_row_prepared["id"] = str(csv_row_prepared["id"])

        # compare with existing DDB item (if any)
        try:
            resp = table.get_item(Key={"id": csv_row_prepared["id"]})
            ddb_item = resp.get("Item")
        except ClientError as e:
            print(f"‚ö†Ô∏è Warning fetching id={csv_row_prepared['id']} from DDB: {e}")
            ddb_item = None

        if ddb_item is None:
            to_write.append(csv_row_prepared)
        else:
            if rows_differ(csv_row_prepared, ddb_item):
                to_write.append(csv_row_prepared)
            else:
                # already up-to-date
                pass

    # --- Batch write to DynamoDB
    uploaded_ids = []
    if to_write:
        print(f"‚¨ÜÔ∏è Writing {len(to_write)} item(s) to DynamoDB...")
        with table.batch_writer(overwrite_by_pkeys=["id"]) as batch:
            count = 0
            for item in to_write:
                # DynamoDB rejects empty strings: convert '' -> None
                safe_item = {k: (None if (isinstance(v, str) and v == "") else v) for k, v in item.items()}
                # convert any floats (should be strings mostly) to Decimal if present
                for k, v in list(safe_item.items()):
                    if isinstance(v, float):
                        if math.isnan(v) or math.isinf(v):
                            safe_item[k] = None
                        else:
                            safe_item[k] = Decimal(str(v))
                batch.put_item(Item=safe_item)
                uploaded_ids.append(safe_item["id"])
                count += 1
                if count % BATCH_PROGRESS_INTERVAL == 0 or count == len(to_write):
                    print(f"‚¨ÜÔ∏è Batch wrote {count}/{len(to_write)}")
    else:
        print("‚ÑπÔ∏è Nothing to write to DynamoDB.")

    # --- Overwrite baseline with incoming file so only exploit_extract.csv remains
    try:
        abs_in = os.path.abspath(current_csv_path)
        abs_base = os.path.abspath(BASELINE_FILE)
        # If incoming file is already the baseline file, do nothing
        if abs_in != abs_base:
            # atomic replace on most OSes (moves file)
            os.replace(current_csv_path, BASELINE_FILE)
            print(f"‚úÖ Baseline replaced: {BASELINE_FILE}")
        else:
            # incoming already at baseline path; ensure it's saved (already is)
            print("‚ÑπÔ∏è Incoming file is already baseline; no file move needed.")
    except Exception as e:
        print(f"‚ùå Failed to set baseline file: {e}")
        raise

    # --- Remove any other dated CSV files if present (safety, but avoids creating new ones)
    # We'll only remove files that match YYYY-MM-DD.csv and are not the baseline file.
    try:
        for fname in os.listdir(DAILY_DIR):
            if fname == os.path.basename(BASELINE_FILE):
                continue
            # pattern: 4-digit year - 2-digit month - 2-digit day .csv
            if len(fname) == 14 and fname[4] == "-" and fname[7] == "-" and fname.endswith(".csv"):
                path = os.path.join(DAILY_DIR, fname)
                try:
                    os.remove(path)
                    print(f"üóëÔ∏è Deleted dated file: {path}")
                except Exception:
                    pass
    except Exception:
        pass

    # --- Save uploaded ids and a sync log (optional)
    timestamp_tag = time.strftime("%Y-%m-%d_%H%M%S")
    if uploaded_ids:
        write_uploaded_ids_file(uploaded_ids, timestamp_tag)

    log = {
        "timestamp": timestamp_tag,
        "total_incoming": new_count,
        "changed_ids_considered": len(changed_ids),
        "to_write": len(to_write),
        "uploaded": len(uploaded_ids),
    }
    log_path = os.path.join(DAILY_DIR, f"sync_log_{timestamp_tag}.json")
    try:
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump(log, f, indent=2)
        print(f"‚ÑπÔ∏è Sync log saved: {log_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to write sync log: {e}")

    return log
